\subsection{Experimental Setup}
\label{sec:setup}

\subsubsection{System used}

We experiment on a system consisting of a $64$-core x86-based AMD EPYC-7742 processor running at $2.25$ GHz. Each core has an L1 cache of $4$ MB, an L2 cache of $32$ MB, and a shared L3 cache of $256$ MB. The server has $512$ GB DDR4 system memory, and runs on Ubuntu $20.04$.


\subsubsection{Configuration}

We use 32-bit integers for vertex ids and 64-bit floating point numbers for vertex ranks. To mark vertices as affected, we use an 8-bit integer vector. The rank computation uses OpenMP's \textit{dynamic schedule} with a chunk size of $2048$, which provides dynamic work-balancing among threads. We use a damping factor of $\alpha = 0.85$ \cite{rank-langville06}, an iteration tolerance of $\tau = 10^{-10}$ \cite{rank-dubey22} using $L\infty$-norm \cite{rank-plimpton11}, and limit the \texttt{MAX\_ITERATIONS} performed to $500$ \cite{nvgraph} for all experiments. We run all experiments with $64$ threads to match the number of cores available on the system (unless specified otherwise). For compilation, we use GCC $9.4$ and OpenMP $5.0$.


\subsubsection{Dataset}

We use four classes of graphs from the \textit{SuiteSparse Matrix Collection} \cite{suite19} in our experiments, shown in Table \ref{tab:dataset}. The number of vertices in these graphs varies from $3.07$ million to $214$ million, and the number of edges varies from $37.4$ million to $1.98$ billion. The presence of dead ends (vertices with no out-links) introduces a global teleport rank contribution that must be computed every iteration. We eliminate this overhead by adding self-loops to all the vertices in the graph (see also \cite{rank-andersen07, rank-langville06}).
%% We also use temporal graph for certain experiments, mention here?

\input{src/tab-dataset}


\subsubsection{Batch Generation}
\label{sec:batch-generation}

%% This needs to be updated
We take each base (static) graph from the dataset and generate a random batch update consisting of an equal mix of edge deletions and insertions. To prepare the set of edges deleted, we delete each existing edge with a uniform probability. We prepare the set of edges to insert by choosing pairs of vertices with equal probability. For the sake of simplicity, we ensure that no new vertices are added to or removed from the graph. We measure the batch size as a fraction of the total number of edges in the original graph and adjust it from $10^{-8}$ to $0.1$ (i.e., $10^{-8}|E|$ to $0.1|E|$), and generate multiple batches for each batch size (for averaging). Along with each batch update, we add self-loops to all vertices.


\subsubsection{Measurement}
\label{sec:measurement}

We measure the time taken by each approach on the updated graph entirely, including any preprocessing cost and the time taken to detect convergence, but exclude time taken for memory allocation and de-allocation. The average time taken by a given method at a given batch size is obtained by taking the geometric mean of time taken for the same batch size for each of the different input graphs. Accordingly, the average speedup is the ratio of these. Further, we measure the error/accuracy of a given approach by measuring the $L\infty$-norm of the ranks produced with respect to ranks obtained from a reference \textit{Static With-barrier} PageRank run on the updated graph with a very low tolerance of $\tau = 10^{-100}$ (limited to $500$ iterations).




\subsection{Performance of DF-PageRank}

We first study the performance of \FroWbar{} and \FroBarf{}  on batch updates of size $10^{-8} |E|$ to $0.1 |E|$, in the absence of faults, and compare them with \StaWbar{}, \StaBarf{}, \NaiWbar{}, and \NaiBarf{}. Figure \ref{fig:no-failure} plots the runtime of the six mentioned approaches. In Figure \ref{fig:no-failure}, solid lines represent the runtime of the barrier-free algorithms and dashed lines represent the runtime of the corresponding algorithms using a barrier. In addition to the runtime of the above algorithms on each instance from Table \ref{tab:dataset} shown in Figure \ref{fig:no-failure-all}, the mean runtime of each algorithm is shown in Figure \ref{fig:no-failure-am-time}. The labels on the line corresponding to \FroBarf{} indicate its speedup over \NaiBarf{}.

In Figure \ref{fig:no-failure-am-time}, we demonstrate that our proposed \FroBarf{}  is faster than \NaiBarf{} until a batch size of $10^{-3} |E|$, with an average speedup of $4.6\times$. From a batch size of $10^{-3} |E|$ (a batch update consisting of more than a million edge deletions / insertions on a billion-edge graph), the performance drops below \NaiBarf{} and \StaBarf{}. Such large batch sizes result in {\em nearly} all of the vertices getting marked as {\em affected}, and hence the performance drops. Note in Figure \ref{fig:no-failure-all} that \FroBarf{} performs well on \textit{road networks} and \textit{protein k-mer graphs}, good enough on \textit{web graphs}, but poorly on \textit{social networks}. This seems to be associated to sparsity of the graphs as \FroBarf{} performing well on sparse graphs.

\input{src/fig-insertions-runtime}
\input{src/fig-insertions-speedup}
\input{src/fig-insertions-error}
\input{src/fig-deletions-runtime}
\input{src/fig-deletions-speedup}
\input{src/fig-deletions-error}
\input{src/fig-8020-runtime}
\input{src/fig-8020-speedup}
\input{src/fig-8020-error}
\input{src/fig-measure-affected}




\paragraph{Accuracy of DF-PageRank}

The error with respect to reference PageRank for \FroBarf{} (as well as \FroWbar{}), as shown in Figure \ref{fig:no-failure-am-error}, starts increasing from $5\times10^{-10}$ to $9\times10^{-10}$ at a batch size of $10^{-6} |E|$ to $10^{-4} |E|$, and then drops back at a batch size of $10^{-2} |E|$. \textit{Dynamic Frontier} approach picks only outgoing neighbors of those vertices to mark whose rank changes by at least a frontier tolerance of $\tau' = \tau/1000$ (where tolerance $\tau = 10^{-10}$). The error falls back because most vertices are getting marked as affected with increasing batch size.  Note that the error with \FroBarf{} lies within the acceptable range of $[0, 10^{-9})$ for a tolerance of $\tau = 10^{-10}$. There is no extra overhead introduced by our barrier-free approach in the absence of faults.
% Note that the slowdown of the \textit{Static} PageRank at a batch size of $0.1 |E|$ can be attributed to the difference in the size of the modified graph. This is because \textit{Dynamic Frontier} approach misses to mark some vertices as affected.

% \input{src/fig-fault-none}



\subsection{Comparing Performance of DF-PageRank}

\subsection{Analyzing Performance of DF-PageRank}

Task-wise split of DF-PageRank with varying batch sizes.

\subsection{Strong Scaling of DF-PageRank}

We next study the strong-scaling behavior of \FroWbar{} and \FroBarf{} on batch updates of a fixed size of $10^{-4} |E|$ in the absence of faults. Here, we measure the speedup of each algorithm with an increasing number of threads from $1$ to $64$ in multiples of $2$ with respect to a single-threaded execution of the algorithm. We additionally compare \StaWbar{}, \StaBarf{}, \NaiWbar{}, and \NaiBarf{}.

We observe from Figure \ref{fig:strong-scaling} that all algorithms exhibit a good decrease in their run time over increasing number of threads. With $64$ threads, \StaBarf{}, \NaiBarf{}, and \FroBarf{} offers an average speedup of $21\times$, while \StaWbar{}, \NaiWbar{}, and \FroWbar{} offer an average speedup of $15\times$ over a single thread.
% Thus, \FroBarf{} offers good scaling performance.


\subsection{Weak Scaling of DF-PageRank}

\subsection{Behavior of DF-PageRank on Temporal Graphs}
