The popularity of PageRank as a centrality metric stems from its ability to assign importance to graph nodes based on their neighbors and scores. In recent years, there has been increasing research focus on computing PageRank for dynamic graphs, where the graphs evolve due to edge insertions and deletions. Simultaneously, the trend of increasing core count in multicore architectures has raised concerns regarding random thread delays and failures. To tackle these challenges, fault-tolerant algorithms that can efficiently function even in the presence of thread delays or crashes are essential.

In this paper we presented the design of a lock-free fault-tolerant PageRank algorithm in the batch dynamic setting, where edge deletions and insertions are applied to the given dynamic graph in batches. First, we introduced our \textit{Static Barrier-free} PageRank (\StaBarf{}) which tolerates random thread delays or crashes. It is simpler in design compared to Eedi et al.'s \textit{Wait-free} version of \textit{Barrier-free} PageRank \cite{rank-eedi22}, and exhibits a $14\%$ improvement in speed to Eedi et al.'s \textit{No-Sync} version of \textit{Barrier-free} PageRank (which is not tolerant to thread crashes). We then adapted our improved \textit{Barrier-free} PageRank to two well-known dynamic PageRank algorithms, namely, \textit{Naive-dynamic} and \textit{Dynamic Traversal}. However, our results indicated that \textit{Dynamic Traversal} approach does not have good performance.

Next, we discussed our \textit{Dynamic Frontier} approach, which incrementally identifies affected vertices that are likely to change their ranks with minimal overhead, in the batch dynamic setting. We integrated it with our fault-tolerant \textit{Barrier-free} PageRank (\FroBarf{}). \FroBarf{} is fast and lock-free. Experimental results indicate that \FroBarf{} is, on average, $4.6\times$ faster than \textit{Naive-dynamic Barrier-free} PageRank (\NaiBarf{}). Simulation studies further show that \FroBarf{} maintains good performance in the presence of random thread delays and can tolerate random thread crashes. In future, we plan to study how to tolerate arbitrary hardware faults in other hardware as well, such as GPUs and ML accelerators.
% We anticipate this to be useful in systems which operate in harsh environments. In the future, we would like to explore the applicability of the proposed technique to other graph algorithms such as betweenness centrality.
