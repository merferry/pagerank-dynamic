In conclusion, this study addresses the design of an optimized parallel implementation of the Leiden algorithm \cite{com-traag19}, a high-quality community detection algorithm that improves upon the popular Louvain method \cite{com-blondel08}, in the shared memory setting. We extend optimizations for the Louvain algorithm \cite{sahu2023gvelouvain} to our implementation of the Leiden algorithm, and use a greedy refinement phase where vertices greedily optimize for delta-modularity within their community bounds, which we observe, offers both better performance and quality than a randomized approach.

On a system equipped with two 16-core Intel Xeon Gold 6226R processors, our implementation of the Leiden algorithm, referred to as GVE-Leiden, attains a processing rate of $352 M$ edges per second on a $3.8 B$ edge graph. It surpasses the original Leiden implementation, igraph Leiden, and NetworKit Leiden by factors of $373\times$, $86\times$, and $7.2\times$ respectively. GVE-Leiden identifies communities of equivalent quality to the first two implementations, and $26\%$ higher quality than NetworKit. Doubling the number of threads results in an average performance scaling of $1.6\times$ for GVE-Leiden. In comparison to GVE-Louvain \cite{sahu2023gvelouvain}, our parallel Louvain implementation, GVE-Leiden reduces internally-disconnected communities by a factor of $11$ with only a $36\%$ increase in runtime.

However, on average, communities identified by GVE-Leiden exhibit $88\times$, $145\times$, and $0.76\times$ more disconnected communities compared to the original Leiden, igraph Leiden, and NetworKit Leiden, respectively. This disparity is particularly noticeable on \textit{social networks}, \textit{road networks}, and \textit{protein k-mer graphs}. Despite this drawback, a potential solution is to address the issue by post-processing, where disconnected communities obtained from GVE-Leiden can be split. We acknowledge this concern, and plan to explore solutions in future work.




%% PageRank conclusion
The popularity of PageRank as a centrality metric stems from its ability to assign importance to graph nodes based on their neighbors and scores. In recent years, there has been increasing research focus on computing PageRank for dynamic graphs, where the graphs evolve due to edge insertions and deletions. Simultaneously, the trend of increasing core count in multicore architectures has raised concerns regarding random thread delays and failures. To tackle these challenges, fault-tolerant algorithms that can efficiently function even in the presence of thread delays or crashes are essential.

In this paper we presented the design of a lock-free fault-tolerant PageRank algorithm in the batch dynamic setting, where edge deletions and insertions are applied to the given dynamic graph in batches. First, we introduced our \textit{Static Barrier-free} PageRank (\StaBarf{}) which tolerates random thread delays or crashes. It is simpler in design compared to Eedi et al.'s \textit{Wait-free} version of \textit{Barrier-free} PageRank \cite{rank-eedi22}, and exhibits a $14\%$ improvement in speed to Eedi et al.'s \textit{No-Sync} version of \textit{Barrier-free} PageRank (which is not tolerant to thread crashes). We then adapted our improved \textit{Barrier-free} PageRank to two well-known dynamic PageRank algorithms, namely, \textit{Naive-dynamic} and \textit{Dynamic Traversal}. However, our results indicated that \textit{Dynamic Traversal} approach does not have good performance.

Next, we discussed our \textit{Dynamic Frontier} approach, which incrementally identifies affected vertices that are likely to change their ranks with minimal overhead, in the batch dynamic setting. We integrated it with our fault-tolerant \textit{Barrier-free} PageRank (\FroBarf{}). \FroBarf{} is fast and lock-free. Experimental results indicate that \FroBarf{} is, on average, $4.6\times$ faster than \textit{Naive-dynamic Barrier-free} PageRank (\NaiBarf{}). Simulation studies further show that \FroBarf{} maintains good performance in the presence of random thread delays and can tolerate random thread crashes. In future, we plan to study how to tolerate arbitrary hardware faults in other hardware as well, such as GPUs and ML accelerators.
% We anticipate this to be useful in systems which operate in harsh environments. In the future, we would like to explore the applicability of the proposed technique to other graph algorithms such as betweenness centrality.
